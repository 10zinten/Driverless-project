{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSD Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_vars = 7\n",
    "num_classes = 3\n",
    "num_anchors = 5\n",
    "batch_size = 3\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "def tf_print(tensor):\n",
    "    print(sess.run(tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1.]\n",
      "  [1. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 0. 0.]]]\n",
      "[[[ 1.9740728   0.00531998  0.05073944  1.9181988 ]\n",
      "  [ 0.9836687   0.70806366  0.47138602 -0.11364946]\n",
      "  [-0.7118693  -0.9339045   0.21746619  1.0575892 ]\n",
      "  [-0.26124918 -0.02741667 -1.823249    0.07471395]\n",
      "  [-1.289948    1.1268674   1.3789265  -0.07503923]]\n",
      "\n",
      " [[ 1.65405     2.0445502  -1.3374531  -0.706513  ]\n",
      "  [-0.24702965  1.5699501   2.0597308  -0.26911816]\n",
      "  [-0.37924895 -1.8115596  -0.7363666   1.2832676 ]\n",
      "  [-0.89154685  1.0267923   0.44763702 -0.05752175]\n",
      "  [-1.6820769  -0.88885796 -0.07986308  0.46741524]]\n",
      "\n",
      " [[-1.7251089   1.1858717  -2.1313174   3.0536988 ]\n",
      "  [ 0.3876682   0.818881    0.3098628   1.6061039 ]\n",
      "  [-0.7169859   0.2832111  -0.21452603  1.2144817 ]\n",
      "  [-0.33307076 -0.4277424  -1.0602584   0.8980847 ]\n",
      "  [ 1.0648307  -1.0960479   1.1318891  -0.932937  ]]]\n"
     ]
    }
   ],
   "source": [
    "# Classificatin tensor\n",
    "# shape: (batch_size, num_anchors, num_classes)\n",
    "# create random label input\n",
    "gt_cl = []\n",
    "for i in range(batch_size):\n",
    "    x = np.eye(num_classes)\n",
    "    # select row at random choice\n",
    "    gt_cl.append(x[np.random.choice(x.shape[0], size=num_anchors)].tolist())\n",
    "gt_cl = tf.constant(gt_cl)\n",
    "\n",
    "\n",
    "# Localization tensor\n",
    "# shape: (batch_size, num_anchors, 4) \n",
    "gt_loc = tf.random_normal(shape=[batch_size, num_anchors, 4])\n",
    "\n",
    "# Senity check\n",
    "tf_print(gt_cl)\n",
    "tf_print(gt_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## match_counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of anchors per batch\n",
      "Total: \n",
      " [5 5 5]\n",
      "Negative (bg=1): \n",
      " [1 4 3]\n",
      "Positive (bg!=1): \n",
      " [4 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Number of anchors per sample\n",
    "# Shape: (batch_size) \n",
    "total_num = tf.ones([batch_size], dtype=tf.int64) * tf.to_int64(num_anchors)\n",
    "\n",
    "# Number of negative (not-matched) anchors per sample, computed by  \n",
    "# counting boxes of the background class in each sample.            \n",
    "# Shape: (batch_size) \n",
    "negatives_num = tf.count_nonzero(gt_cl[:, :, -1], axis=1)\n",
    "\n",
    "# Number of positive (matched) anchors per sample                   \n",
    "# Shape: (batch_size)\n",
    "positives_num = total_num - negatives_num\n",
    "\n",
    "# senity check\n",
    "print(\"Number of anchors per batch\")\n",
    "print(\"Total: \\n\", sess.run(total_num))\n",
    "print(\"Negative (bg=1): \\n\", sess.run(negatives_num))\n",
    "print(\"Positive (bg!=1): \\n\",sess.run(positives_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 1. 2.]\n"
     ]
    }
   ],
   "source": [
    "# Number of positives per sample that is division-safe              \n",
    "# Shape: (batch_size)                                               \n",
    "positives_num_safe = tf.where(tf.equal(positives_num, 0),           \n",
    "                              tf.ones([batch_size])*10e-15,         \n",
    "                              tf.to_float(positives_num))\n",
    "tf_print(positives_num_safe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## match_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive mask:\n",
      "[[False  True  True  True  True]\n",
      " [ True False False False False]\n",
      " [False False  True False  True]]\n",
      "Negative mask:\n",
      "[[ True False False False False]\n",
      " [False  True  True  True  True]\n",
      " [ True  True False  True False]]\n"
     ]
    }
   ],
   "source": [
    "# Boolean tensor determining whether an anchor is a positive        \n",
    "# Shape: (batch_size, num_anchors)  \n",
    "positives_mask = tf.equal(gt_cl[:, :, -1], 0)\n",
    "print(\"Positive mask:\")\n",
    "print(sess.run(positives_mask))\n",
    "\n",
    "# Boolean tensor determining whether an anchor is a negative        \n",
    "negatives_mask = tf.logical_not(positives_mask)\n",
    "print(\"Negative mask:\")\n",
    "tf_print(negatives_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confidence_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39282098 0.4600574  0.01153591 0.1456844  0.85366166]\n",
      " [0.33491522 0.18863396 0.18936157 0.18933313 0.09754541]\n",
      " [0.71655554 0.89165634 0.28296348 0.45780018 0.41719255]]\n"
     ]
    }
   ],
   "source": [
    "# Cross-entorpy tensor  (gt_cl => logits)                                        \n",
    "# Shape: (batch_size, num_anchors) \n",
    "conf = tf.constant([[0.39282098, 0.4600574,  0.01153591, 0.14568441, 0.85366163],\n",
    "                    [0.33491521, 0.18863397, 0.18936157, 0.18933312, 0.09754541],\n",
    "                    [0.71655552, 0.89165631, 0.28296348, 0.45780019, 0.41719255]])\n",
    "tf_print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.4600574  0.01153591 0.1456844  0.85366166]\n",
      " [0.33491522 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.28296348 0.         0.41719255]]\n"
     ]
    }
   ],
   "source": [
    "# Sum up the loss of all the positive anchors                       \n",
    "# Positives - the loss of neg anchors is zeroed out                 \n",
    "# Shape: (batch_size, num_anchors) \n",
    "positives_conf = tf.where(positives_mask, conf, tf.zeros_like(conf))\n",
    "tf_print(positives_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4709394  0.33491522 0.70015603]\n"
     ]
    }
   ],
   "source": [
    "# Total loss of positive anchors                                    \n",
    "# Shape: (batch_size)                                               \n",
    "positives_sum = tf.reduce_sum(positives_conf, axis=-1)\n",
    "tf_print(positives_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39282098 0.         0.         0.         0.        ]\n",
      " [0.         0.18863396 0.18936157 0.18933313 0.09754541]\n",
      " [0.71655554 0.89165634 0.         0.45780018 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Find neg anchors with highest conf loss                           \n",
    "# Negatives - the loss of positive anchor is zeroed out             \n",
    "# Shape: (batch_size, num_anchors)                                  \n",
    "negatives_conf = tf.where(negatives_mask, conf, tf.zeros_like(conf))\n",
    "tf_print(negatives_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39282098 0.         0.         0.         0.        ]\n",
      " [0.18936157 0.18933313 0.18863396 0.09754541 0.        ]\n",
      " [0.89165634 0.71655554 0.45780018 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Top neg - sorted conf loss with highest one first                 \n",
    "# Shape: (batch_size, num_anchors)\n",
    "negatives_top = tf.nn.top_k(negatives_conf, num_anchors)[0]\n",
    "tf_print(negatives_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 3]\n"
     ]
    }
   ],
   "source": [
    "# Find num of negs we want to keep are                              \n",
    "# Max num of negs to keep per sample - keep 3 time as many as pos   \n",
    "# anchors in the sample                                             \n",
    "# Shape: (batch_size) \n",
    "negatives_num_max = tf.minimum(negatives_num, 3*positives_num)\n",
    "tf_print(negatives_num_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [3]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "# mask out superfluous negs and compute the sum of the loss         \n",
    "# Transposed vector of maximum negs per sample                      \n",
    "# Shape: (batch_size, 1)                                            \n",
    "negatives_num_max_t = tf.expand_dims(negatives_num_max, 1)\n",
    "tf_print(negatives_num_max_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# Range tensor: [0, 1, 2, ..., num_anchors-1]                       \n",
    "# Shape: (num_anchors)                                              \n",
    "rng = tf.range(0, num_anchors, 1)\n",
    "tf_print(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2 3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Row range, int64, row of a matrix                                 \n",
    "# shape: (1, num_anchors)                                           \n",
    "range_row = tf.to_int64(tf.expand_dims(rng, 0)) \n",
    "tf_print(range_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True False False False False]\n",
      " [ True  True  True False False]\n",
      " [ True  True  True False False]]\n"
     ]
    }
   ],
   "source": [
    "# Mask of maximum negatives - first `negative_num_max` elements     \n",
    "# in corresponding row are `True`, the rest is false                \n",
    "# Shape: (batch_size, num_anchors)\n",
    "negatives_max_mask = tf.less(range_row, negatives_num_max_t)\n",
    "tf_print(negatives_max_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39282098 0.         0.         0.         0.        ]\n",
      " [0.18936157 0.18933313 0.18863396 0.         0.        ]\n",
      " [0.89165634 0.71655554 0.45780018 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Max negatives - all the positives and superfluous negatives are zeroed out.                                                       \n",
    "# Shape: (batch_size, num_anchors)                     \n",
    "negatives_max = tf.where(negatives_max_mask, negatives_top, tf.zeros_like(negatives_top))\n",
    "tf_print(negative_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.39282098 0.5673287  2.0660121 ]\n"
     ]
    }
   ],
   "source": [
    "# Sum of max negatives for each sample                              \n",
    "# Shape: (batch_size)                                               \n",
    "negatives_max_sum = tf.reduce_sum(negatives_max, axis=-1) \n",
    "tf_print(negatives_max_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the confidence loss for each element "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8637604 0.9022439 2.766168 ]\n"
     ]
    }
   ],
   "source": [
    "# Total confidence loss for each sample                             \n",
    "# Shape: (batch_size) \n",
    "confidence_loss = tf.add(positives_sum, negatives_max_sum)\n",
    "tf_print(confidence_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8440574\n"
     ]
    }
   ],
   "source": [
    "# Mean confidence loss for the batch                                \n",
    "# Shape: scalar                                                     \n",
    "batch_confidence_loss = tf.reduce_mean(confidence_loss, name='confidence_loss')\n",
    "tf_print(batch_confidence_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
